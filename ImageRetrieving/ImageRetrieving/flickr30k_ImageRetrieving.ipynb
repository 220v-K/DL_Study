{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings & Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_SlTxSx8D5t2",
    "outputId": "002a9683-2e8e-464c-f2d1-e6c57bc07d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/bin/python\n",
      "Name: torch\n",
      "Version: 2.0.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /opt/anaconda3/lib/python3.11/site-packages\n",
      "Requires: filelock, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: \n",
      "cuda 버전: 12.1\n",
      "/usr/local/cuda-12.2/bin:/usr/local/cuda-12.2/bin:/usr/local/cuda-12.2/bin:/usr/local/cuda-12.2/bin:/home/gpu_04/.vscode-server/cli/servers/Stable-fabdb6a30b49f79a7aba0f2ad9df9b399473380f/server/bin/remote-cli:/opt/anaconda3/bin:/opt/anaconda3/bin:/usr/local/cuda-12.2/bin:/opt/anaconda3/condabin:/usr/local/cuda-12.2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/gpu_04/.vscode-server/data/User/globalStorage/github.copilot-chat/debugCommand\n",
      "/usr/local/cuda-12.2/lib64\n"
     ]
    }
   ],
   "source": [
    "# 2. 필요한 라이브러리 설치\n",
    "# %pip install -q kaggle\n",
    "\n",
    "# 3. Kaggle API 설정\n",
    "# import os\n",
    "\n",
    "!which python\n",
    "!pip show torch\n",
    "import torch\n",
    "print(\"cuda 버전:\", torch.version.cuda)\n",
    "!echo $PATH\n",
    "!echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 버전: 2.5.1+cu121\n",
      "CUDA 사용 가능 여부: True\n",
      "현재 디바이스: NVIDIA RTX A6000\n",
      "CUDA 버전: 12.1\n",
      "Allocated memory: 0.00 MB\n",
      "Reserved memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch 버전:\", torch.__version__)\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "print(\"현재 디바이스:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"CUDA 버전:\", torch.version.cuda if torch.cuda.is_available() else \"None\")\n",
    "\n",
    "# GPU 메모리 단편화 문제 완화\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# GPU 캐시 비우기\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# 1. 불필요한 변수 삭제\n",
    "# del variable\n",
    "\n",
    "# 2. 가비지 컬렉터 실행\n",
    "gc.collect()\n",
    "\n",
    "# 3. PyTorch 캐시 메모리 해제\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 4. 메모리 사용 상태 출력\n",
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Reserved memory: {torch.cuda.memory_reserved() / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu_04/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-26 23:36:24.737148: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740580584.759778 2777379 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740580584.766639 2777379 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 23:36:24.790609: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from transformers import ViTModel, RobertaModel, RobertaTokenizer\n",
    "# swin 모델는 transformers에서 불러올 수 있음\n",
    "from transformers import SwinModel\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 시드 고정\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet Structure (Lightning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모든 데이터셋 쌍 (이미지-캡션 5개) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. Flickr30k 데이터셋을 Multi-Caption Dataset으로 변환\n",
    "#    - 원본 record 하나가 한 이미지와 5개의 캡션을 가지므로,\n",
    "#      각 record에 대해 (record_idx, caption_idx)로 mapping하는 인덱스를 생성합니다.\n",
    "# =============================================================================\n",
    "class Flickr30KMultiCaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    한 이미지 record의 모든 캡션(총 5개)을 각각 하나의 예제로 반환.\n",
    "    각 예제는 이미지, 텍스트 토큰, attention mask와 함께 원본 이미지의 인덱스(img_id)를 포함.\n",
    "    \"\"\"\n",
    "    # def __init__(self, hf_dataset, tokenizer, image_transform, max_length=64):\n",
    "    #     self.hf_dataset = hf_dataset\n",
    "    #     self.tokenizer = tokenizer\n",
    "    #     self.image_transform = image_transform\n",
    "    #     self.max_length = max_length\n",
    "    #     self.index_map = []  # (record_idx, caption_idx)\n",
    "    #     for rec_idx, record in enumerate(self.hf_dataset):\n",
    "    #         captions = record[\"caption\"]\n",
    "    #         for cap_idx in range(len(captions)):\n",
    "    #             self.index_map.append((rec_idx, cap_idx))\n",
    "    def __init__(self, hf_dataset, tokenizer, image_transform, max_length=64):\n",
    "        \"\"\"\n",
    "        Dataset for Flickr30K with only one caption per image.\n",
    "        \n",
    "        Args:\n",
    "            hf_dataset: HuggingFace dataset for Flickr30K\n",
    "            tokenizer: Text tokenizer (RoBERTa)\n",
    "            image_transform: Image transformation pipeline\n",
    "            max_length: Maximum token length for captions\n",
    "        \"\"\"\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_transform = image_transform\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # 각 이미지마다 첫 번째 캡션만 사용\n",
    "        # 또는 랜덤하게 하나의 캡션만 선택하려면 아래 주석 처리된 방식 사용\n",
    "        self.index_map = [(idx, 0) for idx in range(len(self.hf_dataset))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record_idx, caption_idx = self.index_map[idx]\n",
    "        record = self.hf_dataset[record_idx]\n",
    "        pil_image = record[\"image\"]\n",
    "        caption = record[\"caption\"][caption_idx]\n",
    "\n",
    "        # 이미지 전처리\n",
    "        image = self.image_transform(pil_image)\n",
    "\n",
    "        # 텍스트 토큰화\n",
    "        tokenized = self.tokenizer(\n",
    "            caption,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"img_id\": record_idx  # 동일 이미지면 동일한 id 부여\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. DataModule 구성 (학습/검증/테스트 셋 분리)\n",
    "# =============================================================================\n",
    "class Flickr30KDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 train_dataset_hf,\n",
    "                 valid_dataset_hf,\n",
    "                 test_dataset_hf,\n",
    "                 batch_size=128,\n",
    "                 num_workers=4,\n",
    "                 max_length=64):\n",
    "        super().__init__()\n",
    "        self.train_dataset_hf = train_dataset_hf\n",
    "        self.valid_dataset_hf = valid_dataset_hf\n",
    "        self.test_dataset_hf = test_dataset_hf\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # 이미지 전처리: Resize, ToTensor, Normalize\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5),\n",
    "                                 std=(0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        # 텍스트 토크나이저: roberta-large 사용\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = Flickr30KMultiCaptionDataset(\n",
    "                self.train_dataset_hf[\"test\"],  # 수정: [\"test\"] 접근\n",
    "                tokenizer=self.tokenizer,\n",
    "                image_transform=self.image_transform,\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "            self.valid_dataset = Flickr30KMultiCaptionDataset(\n",
    "                self.valid_dataset_hf[\"test\"],  # 수정: [\"test\"] 접근\n",
    "                tokenizer=self.tokenizer,\n",
    "                image_transform=self.image_transform,\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = Flickr30KMultiCaptionDataset(\n",
    "                self.test_dataset_hf[\"test\"],  # 수정: [\"test\"] 접근\n",
    "                tokenizer=self.tokenizer,\n",
    "                image_transform=self.image_transform,\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valid_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure (Lightning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. 다중 Positive Contrastive Loss 함수 구현\n",
    "#    - 각 anchor(텍스트 혹은 이미지)마다, 동일 img_id를 가진 모든 다른 샘플들을 positive로 취급.\n",
    "#    - loss = logsumexp(전체) – log(sum(exp(similarity)) over positive)\n",
    "# =============================================================================\n",
    "def multi_positive_contrastive_loss(similarity_matrix):\n",
    "    \"\"\"\n",
    "    표준 InfoNCE 손실 함수.\n",
    "    similarity_matrix: (B, B)\n",
    "    \"\"\"\n",
    "    # ground truth는 대각선 인덱스 (자기 자신과 매칭)\n",
    "    labels = torch.arange(similarity_matrix.size(0), device=similarity_matrix.device)\n",
    "    loss_i2t = F.cross_entropy(similarity_matrix, labels)\n",
    "    loss_t2i = F.cross_entropy(similarity_matrix.t(), labels)\n",
    "    return (loss_i2t + loss_t2i) / 2.0\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Lightning Module 구성\n",
    "#    - 이미지 인코더는 SOTA인 Swin (또는 ViT) 사용, 텍스트 인코더는 roberta-large 사용\n",
    "#    - 인코딩 후 projection layer를 거쳐 L2 normalization 수행\n",
    "#    - forward() 내부에서 이미지 인코더가 swin인 경우에는 평균 풀링으로 feature를 추출\n",
    "#    - training/validation 시, 배치 내 img_id를 활용하여 positive mask를 생성한 후 loss를 계산함\n",
    "# =============================================================================\n",
    "class ImageTextLightningModel(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 image_encoder_name=\"microsoft/swin-base-patch4-window7-224\",\n",
    "                 text_encoder_name=\"roberta-large\",\n",
    "                 embed_dim=256,\n",
    "                 temperature=0.07,\n",
    "                 learning_rate=1e-5,\n",
    "                 vit_train_layers=12,        # 이미지 인코더에서 fine-tuning할 마지막 레이어 수\n",
    "                 roberta_train_layers=12):   # 텍스트 인코더에서 fine-tuning할 마지막 레이어 수\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # 1) 이미지 인코더: swin 또는 ViT 선택\n",
    "        if \"swin\" in image_encoder_name:\n",
    "            self.image_encoder = SwinModel.from_pretrained(image_encoder_name)\n",
    "        else:\n",
    "            self.image_encoder = ViTModel.from_pretrained(image_encoder_name)\n",
    "\n",
    "        # 2) 텍스트 인코더: roberta-large 사용\n",
    "        self.text_encoder = RobertaModel.from_pretrained(text_encoder_name)\n",
    "\n",
    "        # 3) Projection layers (각 인코더의 hidden_size에 맞춰 구성)\n",
    "        image_hidden_size = self.image_encoder.config.hidden_size\n",
    "        text_hidden_size = self.text_encoder.config.hidden_size\n",
    "        self.image_proj = nn.Linear(image_hidden_size, embed_dim)\n",
    "        self.text_proj = nn.Linear(text_hidden_size, embed_dim)\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # 검증/테스트 시 중간 결과 저장용\n",
    "        self._val_outputs = []\n",
    "        self.test_image_embeds = []\n",
    "        self.test_text_embeds = []\n",
    "\n",
    "        # Freeze & Unfreeze: 전체 파라미터 동결 후, 마지막 몇 레이어만 unfreeze\n",
    "        self.freeze_image_encoder_layers(train_layers=vit_train_layers)\n",
    "        self.freeze_roberta_layers(train_layers=roberta_train_layers)\n",
    "\n",
    "    def freeze_image_encoder_layers(self, train_layers):\n",
    "        # 전체 파라미터 동결\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # ViT의 경우: self.image_encoder.encoder.layer, Swin의 경우: self.image_encoder.encoder.layers 로 구성됨\n",
    "        if hasattr(self.image_encoder.encoder, \"layer\"):\n",
    "            layers = self.image_encoder.encoder.layer\n",
    "        elif hasattr(self.image_encoder.encoder, \"layers\"):\n",
    "            layers = self.image_encoder.encoder.layers\n",
    "        else:\n",
    "            layers = []\n",
    "        total_layers = len(layers)\n",
    "        for layer_idx in range(max(0, total_layers - train_layers), total_layers):\n",
    "            for param in layers[layer_idx].parameters():\n",
    "                param.requires_grad = True\n",
    "        # 추가적으로 layernorm이나 pooler가 있다면 unfreeze\n",
    "        if hasattr(self.image_encoder, \"layernorm\"):\n",
    "            for param in self.image_encoder.layernorm.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def freeze_roberta_layers(self, train_layers):\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        total_layers = len(self.text_encoder.encoder.layer)\n",
    "        for layer_idx in range(max(0, total_layers - train_layers), total_layers):\n",
    "            for param in self.text_encoder.encoder.layer[layer_idx].parameters():\n",
    "                param.requires_grad = True\n",
    "        if hasattr(self.text_encoder, \"pooler\"):\n",
    "            for param in self.text_encoder.pooler.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # --- 이미지 인코더 ---\n",
    "        image_outputs = self.image_encoder(pixel_values=images)\n",
    "        # swin인 경우: 평균 풀링, 그 외에는 [CLS] 토큰 사용\n",
    "        if self.hparams.image_encoder_name.startswith(\"microsoft/swin\"):\n",
    "            image_feat = image_outputs.last_hidden_state.mean(dim=1)\n",
    "        else:\n",
    "            image_feat = image_outputs.last_hidden_state[:, 0, :]\n",
    "        image_embeds = self.image_proj(image_feat)\n",
    "        image_embeds = F.normalize(image_embeds, p=2, dim=-1)\n",
    "\n",
    "        # --- 텍스트 인코더 ---\n",
    "        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_feat = text_outputs.last_hidden_state[:, 0, :]\n",
    "        text_embeds = self.text_proj(text_feat)\n",
    "        text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n",
    "\n",
    "        return image_embeds, text_embeds\n",
    "\n",
    "    def compute_contrastive_loss(self, image_embeds, text_embeds, img_ids):\n",
    "        \"\"\"\n",
    "        싱글 캡션 모델에서는 단순히 배치 내 쌍에 대한 대각선 요소를 positive로 취급합니다.\n",
    "        img_ids는 더 이상 필요하지 않지만, 기존 호출 코드와 호환성을 위해 파라미터는 유지합니다.\n",
    "        \"\"\"\n",
    "        # text→image 방향: (B, B) similarity matrix\n",
    "        sim_matrix = torch.matmul(text_embeds, image_embeds.t()) / self.temperature\n",
    "        # 싱글 캡션이므로 단순 InfoNCE 손실 사용\n",
    "        return multi_positive_contrastive_loss(sim_matrix)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images = batch[\"image\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        img_ids = batch[\"img_id\"]\n",
    "        image_embeds, text_embeds = self(images, input_ids, attention_mask)\n",
    "        loss = self.compute_contrastive_loss(image_embeds, text_embeds, img_ids)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        optimizer = self.optimizers()\n",
    "        if isinstance(optimizer, list):\n",
    "            optimizer = optimizer[0]\n",
    "        for i, param_group in enumerate(optimizer.param_groups):\n",
    "            lr = param_group[\"lr\"]\n",
    "            self.log(f\"lr_layer_{i}\", lr, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images = batch[\"image\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        img_ids = batch[\"img_id\"]\n",
    "        image_embeds, text_embeds = self(images, input_ids, attention_mask)\n",
    "        val_loss = self.compute_contrastive_loss(image_embeds, text_embeds, img_ids)\n",
    "        self.log(\"val_loss_step\", val_loss, prog_bar=False)\n",
    "        return {\n",
    "            \"val_loss\": val_loss,\n",
    "            \"image_embeds\": image_embeds,\n",
    "            \"text_embeds\": text_embeds,\n",
    "            \"img_ids\": img_ids\n",
    "        }\n",
    "\n",
    "    def on_validation_batch_end(self, outputs, batch, batch_idx, dataloader_idx=0):\n",
    "        self._val_outputs.append(outputs)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # 평균 validation loss 로깅\n",
    "        val_losses = torch.stack([o[\"val_loss\"] for o in self._val_outputs])\n",
    "        avg_val_loss = val_losses.mean()\n",
    "        self.log(\"val_loss\", avg_val_loss, prog_bar=True)\n",
    "\n",
    "        # 배치별 결과를 모두 모아 recall@K 계산 (text→image retrieval)\n",
    "        all_image_embeds = torch.cat([o[\"image_embeds\"] for o in self._val_outputs], dim=0)\n",
    "        all_text_embeds  = torch.cat([o[\"text_embeds\"]  for o in self._val_outputs], dim=0)\n",
    "        similarity_matrix = torch.matmul(all_text_embeds, all_image_embeds.t())\n",
    "        recall_at_k = self.compute_recall(similarity_matrix, ks=[1,5,10])\n",
    "        for k, v in recall_at_k.items():\n",
    "            self.log(f\"val_recall@{k}\", v, prog_bar=True)\n",
    "        self._val_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images = batch[\"image\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        image_embeds, text_embeds = self(images, input_ids, attention_mask)\n",
    "        return {\"image_embeds\": image_embeds, \"text_embeds\": text_embeds}\n",
    "\n",
    "    def on_test_batch_end(self, outputs, batch, batch_idx, dataloader_idx=0):\n",
    "        self.test_image_embeds.append(outputs[\"image_embeds\"])\n",
    "        self.test_text_embeds.append(outputs[\"text_embeds\"])\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        all_image_embeds = torch.cat(self.test_image_embeds, dim=0)\n",
    "        all_text_embeds  = torch.cat(self.test_text_embeds, dim=0)\n",
    "        similarity_matrix = torch.matmul(all_text_embeds, all_image_embeds.t())\n",
    "        recall_at_k = self.compute_recall(similarity_matrix, ks=[1,5,10])\n",
    "        for k, v in recall_at_k.items():\n",
    "            self.log(f\"test_recall@{k}\", v, prog_bar=True)\n",
    "        print(f\"[on_test_epoch_end] Test Recall: {recall_at_k}\")\n",
    "        self.test_image_embeds.clear()\n",
    "        self.test_text_embeds.clear()\n",
    "\n",
    "    def compute_recall(self, similarity_matrix, ks=[1,5,10]):\n",
    "        device = similarity_matrix.device\n",
    "        n = similarity_matrix.size(0)\n",
    "        # 대각선 요소가 정답 (각 텍스트는 같은 인덱스의 이미지에 매칭)\n",
    "        ground_truth = torch.arange(n, device=device)\n",
    "        sorted_indices = similarity_matrix.argsort(dim=1, descending=True)\n",
    "        recall_scores = {}\n",
    "        for k in ks:\n",
    "            top_k = sorted_indices[:, :k]\n",
    "            match = (top_k == ground_truth.unsqueeze(1)).any(dim=1)\n",
    "            recall_scores[k] = match.float().mean().item()\n",
    "        return recall_scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        base_lr = self.learning_rate\n",
    "        layerwise_decay = 0.9\n",
    "        optimizer_params = []\n",
    "\n",
    "        # --- 이미지 인코더: layer-wise learning rate decay ---\n",
    "        if hasattr(self.image_encoder.encoder, \"layer\"):\n",
    "            image_layers = self.image_encoder.encoder.layer\n",
    "        elif hasattr(self.image_encoder.encoder, \"layers\"):\n",
    "            image_layers = self.image_encoder.encoder.layers\n",
    "        else:\n",
    "            image_layers = []\n",
    "        total_image_layers = len(image_layers)\n",
    "        train_image_layers = self.hparams.vit_train_layers\n",
    "        for i, layer_idx in enumerate(range(max(0, total_image_layers - train_image_layers), total_image_layers)):\n",
    "            lr = base_lr * (layerwise_decay ** i)\n",
    "            optimizer_params.append({\"params\": image_layers[layer_idx].parameters(), \"lr\": lr})\n",
    "\n",
    "        # --- 텍스트 인코더: layer-wise learning rate decay ---\n",
    "        total_text_layers = len(self.text_encoder.encoder.layer)\n",
    "        train_text_layers = self.hparams.roberta_train_layers\n",
    "        for i, layer_idx in enumerate(range(max(0, total_text_layers - train_text_layers), total_text_layers)):\n",
    "            lr = base_lr * (layerwise_decay ** i)\n",
    "            optimizer_params.append({\"params\": self.text_encoder.encoder.layer[layer_idx].parameters(), \"lr\": lr})\n",
    "\n",
    "        # --- Projection layers: 새로 학습되는 부분은 더 큰 lr 적용 ---\n",
    "        optimizer_params.append({\"params\": self.image_proj.parameters(), \"lr\": base_lr * 5})\n",
    "        optimizer_params.append({\"params\": self.text_proj.parameters(), \"lr\": base_lr * 5})\n",
    "\n",
    "        optimizer = torch.optim.AdamW(optimizer_params, lr=base_lr, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.trainer.max_epochs, eta_min=1e-7\n",
    "        )\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6. 데이터셋 로드 및 DataModule 생성\n",
    "# =============================================================================\n",
    "# 허깅페이스 데이터셋 로드 및 split 별로 filtering (split 필드 기준)\n",
    "dataset = load_dataset(\"nlphuji/flickr30k\")\n",
    "train_dataset = dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "valid_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "test_dataset  = dataset.filter(lambda x: x[\"split\"] == \"test\")\n",
    "\n",
    "data_module = Flickr30KDataModule(\n",
    "    train_dataset_hf=train_dataset,\n",
    "    valid_dataset_hf=valid_dataset,\n",
    "    test_dataset_hf=test_dataset,\n",
    "    batch_size=128,\n",
    "    num_workers=4,\n",
    "    max_length=64\n",
    ")\n",
    "data_module.setup(\"fit\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7. 모델 초기화 및 학습 설정\n",
    "# =============================================================================\n",
    "model = ImageTextLightningModel(\n",
    "    image_encoder_name=\"microsoft/swin-base-patch4-window7-224\",\n",
    "    text_encoder_name=\"roberta-large\",\n",
    "    embed_dim=256,\n",
    "    temperature=0.07,\n",
    "    learning_rate=1e-5,\n",
    "    vit_train_layers=0,\n",
    "    roberta_train_layers=0\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=\"ImageRetrieveLogs\",\n",
    "    name=\"ImageRetrieve_MultiPos\"\n",
    ")\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     monitor=\"val_recall@5\",\n",
    "#     mode=\"max\",\n",
    "#     dirpath=\"checkpoints_multi\",\n",
    "#     filename=\"best-checkpoint\",\n",
    "#     save_top_k=3,\n",
    "#     save_last=True\n",
    "# )\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_recall@5\",\n",
    "    patience=5,\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,   # 에폭 수: 하드웨어 및 실험에 따라 조정 가능\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\",\n",
    "    logger=logger,\n",
    "    callbacks=early_stopping_callback,\n",
    "    enable_checkpointing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type         | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | image_encoder | SwinModel    | 86.7 M | eval \n",
      "1 | text_encoder  | RobertaModel | 355 M  | eval \n",
      "2 | image_proj    | Linear       | 262 K  | train\n",
      "3 | text_proj     | Linear       | 262 K  | train\n",
      "-------------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "441 M     Non-trainable params\n",
      "442 M     Total params\n",
      "1,770.511 Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "927       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  26%|▎| 60/227 [00:19<00:54,  3.06it/s, v_num=1, train_loss_step=1.350, val_loss=1.740, val_recall@1=0.294, val_recall@5=0.587, val_recall@10="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:295\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    293\u001b[0m     batch \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_to_device\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch, dataloader_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_progress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mon_batch_start(batch)\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.10/site-packages/pytorch_lightning/loops/progress.py:155\u001b[0m, in \u001b[0;36m_Progress.increment_ready\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `total` and `current` instances should be of the same class\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mincrement_ready\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\u001b[38;5;241m.\u001b[39mready \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 및 체크포인트 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# 저장된 체크포인트 파일 경로\n",
    "checkpoint_path = \"/home/gpu_04/jw2020/ImageRetrieving/checkpoints_7th/best-checkpoint.ckpt\"\n",
    "\n",
    "# 모델 로드\n",
    "model = ImageTextLightningModel.load_from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:01<00:00,  5.48it/s][on_test_epoch_end] Test Recall: {1: 0.5160000324249268, 5: 0.8110000491142273, 10: 0.8860000371932983}\n",
      "Testing DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:01<00:00,  5.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_recall@1       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5160000324249268     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_recall@10       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8860000371932983     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_recall@5       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8110000491142273     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_recall@1      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5160000324249268    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_recall@10      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8860000371932983    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_recall@5      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8110000491142273    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_recall@1': 0.5160000324249268,\n",
       "  'test_recall@5': 0.8110000491142273,\n",
       "  'test_recall@10': 0.8860000371932983}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "trainer.test(model, data_module)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
