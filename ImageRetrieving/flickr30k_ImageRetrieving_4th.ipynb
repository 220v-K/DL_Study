{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings & Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_SlTxSx8D5t2",
    "outputId": "002a9683-2e8e-464c-f2d1-e6c57bc07d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/bin/python\n",
      "Name: torch\n",
      "Version: 2.0.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /opt/anaconda3/lib/python3.11/site-packages\n",
      "Requires: filelock, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: \n",
      "cuda 버전: 12.1\n",
      "/usr/local/cuda-12.2/bin:/usr/local/cuda-12.2/bin:/usr/local/cuda-12.2/bin:/usr/local/cuda-12.2/bin:/home/gpu_04/.vscode-server/cli/servers/Stable-fabdb6a30b49f79a7aba0f2ad9df9b399473380f/server/bin/remote-cli:/opt/anaconda3/bin:/opt/anaconda3/bin:/usr/local/cuda-12.2/bin:/opt/anaconda3/condabin:/usr/local/cuda-12.2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/gpu_04/.vscode-server/data/User/globalStorage/github.copilot-chat/debugCommand\n",
      "/usr/local/cuda-12.2/lib64\n"
     ]
    }
   ],
   "source": [
    "# 2. 필요한 라이브러리 설치\n",
    "# %pip install -q kaggle\n",
    "\n",
    "# 3. Kaggle API 설정\n",
    "# import os\n",
    "\n",
    "!which python\n",
    "!pip show torch\n",
    "import torch\n",
    "print(\"cuda 버전:\", torch.version.cuda)\n",
    "!echo $PATH\n",
    "!echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 버전: 2.5.1+cu121\n",
      "CUDA 사용 가능 여부: True\n",
      "현재 디바이스: NVIDIA RTX A6000\n",
      "CUDA 버전: 12.1\n",
      "Allocated memory: 0.00 MB\n",
      "Reserved memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch 버전:\", torch.__version__)\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "print(\"현재 디바이스:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"CUDA 버전:\", torch.version.cuda if torch.cuda.is_available() else \"None\")\n",
    "\n",
    "# GPU 메모리 단편화 문제 완화\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# GPU 캐시 비우기\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# 1. 불필요한 변수 삭제\n",
    "# del variable\n",
    "\n",
    "# 2. 가비지 컬렉터 실행\n",
    "gc.collect()\n",
    "\n",
    "# 3. PyTorch 캐시 메모리 해제\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 4. 메모리 사용 상태 출력\n",
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Reserved memory: {torch.cuda.memory_reserved() / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Flickr30k 데이터셋 다운로드\n",
    "dataset = load_dataset(\"nlphuji/flickr30k\")\n",
    "train_dataset = dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "valid_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "test_dataset = dataset.filter(lambda x: x[\"split\"] == \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
      "        num_rows: 29000\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
      "        num_rows: 1014\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(valid_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import random\n",
    "\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel, RobertaModel, RobertaTokenizer\n",
    "\n",
    "# 시드 고정\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet Structure (Lightning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모든 데이터셋 쌍 (이미지-캡션 5개) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr30KCustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Flickr30K에서 한 이미지당 최대 5개 캡션을 모두 사용하여\n",
    "    (이미지, 캡션) 쌍을 중복 생성해 총 5배의 데이터로 만든다.\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_dataset, tokenizer, image_transform, max_length=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_transform = image_transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.pairs = []  # (PIL.Image, caption) 쌍 리스트\n",
    "        for item in hf_dataset:\n",
    "            pil_image = item[\"image\"]\n",
    "            captions = item[\"caption\"]  # 최대 5개\n",
    "            # 5개 캡션 각각에 대해 중복 샘플링\n",
    "            for c in captions:\n",
    "                self.pairs.append((pil_image, c))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pil_image, caption = self.pairs[idx]\n",
    "\n",
    "        # 1) 이미지 변환\n",
    "        image = self.image_transform(pil_image)\n",
    "\n",
    "        # 2) 캡션 토큰화 (RoBERTa)\n",
    "        tokenized = self.tokenizer(\n",
    "            caption,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"].squeeze(0)       # (seq_len,)\n",
    "        attention_mask = tokenized[\"attention_mask\"].squeeze(0)  # (seq_len,)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,  # (3, 224, 224)\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqueImageBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    같은 이미지가 한 배치 내에 두 번 포함되지 않도록 하는 Batch Sampler\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.image_to_indices = {}\n",
    "\n",
    "        # 같은 이미지를 가진 인덱스를 그룹화\n",
    "        for idx, (image, _) in enumerate(dataset.pairs):\n",
    "            image_id = id(image)  # 이미지 객체의 ID로 그룹화\n",
    "            if image_id not in self.image_to_indices:\n",
    "                self.image_to_indices[image_id] = []\n",
    "            self.image_to_indices[image_id].append(idx)\n",
    "\n",
    "        # 이미지별 그룹 리스트\n",
    "        self.image_groups = list(self.image_to_indices.values())\n",
    "\n",
    "        # 모든 인덱스를 하나의 리스트로 모으고 섞음\n",
    "        self.indices = [idx for group in self.image_groups for idx in group]\n",
    "        self.num_batches = len(self.indices) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        # 샘플 순서를 무작위로 섞은 후 배치를 만듦\n",
    "        indices = self.indices.copy()\n",
    "        random.shuffle(indices)\n",
    "        for i in range(self.num_batches):\n",
    "            batch = indices[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel, RobertaModel, RobertaTokenizer\n",
    "\n",
    "class Flickr30KDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 train_dataset_hf,\n",
    "                 valid_dataset_hf,\n",
    "                 test_dataset_hf,\n",
    "                 batch_size=32,\n",
    "                 num_workers=4):\n",
    "        super().__init__()\n",
    "        self.train_dataset_hf = train_dataset_hf\n",
    "        self.valid_dataset_hf = valid_dataset_hf\n",
    "        self.test_dataset_hf = test_dataset_hf\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        # 이미지 전처리 (ViT를 위한 224x224)\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5),\n",
    "                                 std=(0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "        # RoBERTa 토크나이저\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        - train_dataset_hf / valid_dataset_hf / test_dataset_hf 는\n",
    "          이미 split 기준으로 필터링된 DatasetDict 형태일 것이라 가정.\n",
    "        \"\"\"\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = Flickr30KCustomDataset(\n",
    "                self.train_dataset_hf[\"test\"],  # 실제 train split\n",
    "                tokenizer=self.tokenizer,\n",
    "                image_transform=self.image_transform\n",
    "            )\n",
    "            self.valid_dataset = Flickr30KCustomDataset(\n",
    "                self.valid_dataset_hf[\"test\"],\n",
    "                tokenizer=self.tokenizer,\n",
    "                image_transform=self.image_transform\n",
    "            )\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = Flickr30KCustomDataset(\n",
    "                self.test_dataset_hf[\"test\"],\n",
    "                tokenizer=self.tokenizer,\n",
    "                image_transform=self.image_transform\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # 한 batch에서 같은 이미지가 중복되지 않도록 UniqueImageBatchSampler 사용\n",
    "        train_sampler = UniqueImageBatchSampler(self.train_dataset, batch_size=self.batch_size)\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            # batch_size=self.batch_size,   #! batch_sampler option is mutually exclusive with batch_size, shuffle, sampler, and drop_last\n",
    "            # shuffle=True,\n",
    "            batch_sampler=train_sampler,    # batch_sampler\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valid_sampler = UniqueImageBatchSampler(self.valid_dataset, batch_size=self.batch_size)\n",
    "        return DataLoader(\n",
    "            self.valid_dataset,\n",
    "            # batch_size=self.batch_size,\n",
    "            # shuffle=False,\n",
    "            batch_sampler=valid_sampler,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure (Lightning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextLightningModel(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 image_encoder_name=\"google/vit-base-patch16-224\",\n",
    "                 text_encoder_name=\"roberta-base\",\n",
    "                 embed_dim=256,\n",
    "                 temperature=0.07,\n",
    "                 learning_rate=5e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # 1) Image Encoder (ViT)\n",
    "        self.image_encoder = ViTModel.from_pretrained(image_encoder_name)\n",
    "\n",
    "        # 2) Text Encoder (RoBERTa)\n",
    "        self.text_encoder = RobertaModel.from_pretrained(text_encoder_name)\n",
    "\n",
    "        # 3) Projection layers: 768 -> embed_dim\n",
    "        self.image_proj = nn.Linear(768, embed_dim)\n",
    "        self.text_proj = nn.Linear(768, embed_dim)\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # (테스트 시) 임베딩 저장 버퍼\n",
    "        self.test_image_embeds = []\n",
    "        self.test_text_embeds = []\n",
    "        \n",
    "        # Validation 결과 저장 버퍼\n",
    "        self._val_outputs = []\n",
    "        \n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # --- 이미지 임베딩 ---\n",
    "        image_outputs = self.image_encoder(pixel_values=images)\n",
    "        image_cls = image_outputs.last_hidden_state[:, 0, :]\n",
    "        image_embeds = self.image_proj(image_cls)\n",
    "        image_embeds = F.normalize(image_embeds, p=2, dim=-1)\n",
    "\n",
    "        # --- 텍스트 임베딩 ---\n",
    "        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_cls = text_outputs.last_hidden_state[:, 0, :]\n",
    "        text_embeds = self.text_proj(text_cls)\n",
    "        text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n",
    "\n",
    "        return image_embeds, text_embeds\n",
    "\n",
    "    def compute_contrastive_loss(self, image_embeds, text_embeds):\n",
    "        \"\"\"\n",
    "        Symmetric InfoNCE Loss\n",
    "        \"\"\"\n",
    "        logits_per_image = image_embeds @ text_embeds.t() / self.temperature\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        batch_size = image_embeds.size(0)\n",
    "        labels = torch.arange(batch_size, device=self.device)\n",
    "\n",
    "        loss_i = F.cross_entropy(logits_per_image, labels)\n",
    "        loss_t = F.cross_entropy(logits_per_text, labels)\n",
    "        loss = (loss_i + loss_t) / 2.0\n",
    "        return loss\n",
    "\n",
    "    # -----------------------\n",
    "    # Training\n",
    "    # -----------------------\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images = batch[\"image\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "        image_embeds, text_embeds = self(images, input_ids, attention_mask)\n",
    "        loss = self.compute_contrastive_loss(image_embeds, text_embeds)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    # -----------------------\n",
    "    # Validation\n",
    "    # -----------------------\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images = batch[\"image\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "        image_embeds, text_embeds = self(images, input_ids, attention_mask)\n",
    "        val_loss = self.compute_contrastive_loss(image_embeds, text_embeds)\n",
    "        self.log(\"val_loss_step\", val_loss, prog_bar=False, on_epoch=False, on_step=True)\n",
    "\n",
    "        return {\n",
    "            \"val_loss\": val_loss,\n",
    "            \"image_embeds\": image_embeds,\n",
    "            \"text_embeds\": text_embeds\n",
    "        }\n",
    "    \n",
    "    def on_validation_batch_end(self, outputs, batch, batch_idx, dataloader_idx=0):\n",
    "        \"\"\"\n",
    "        validation_step에서 반환된 outputs를 모아서\n",
    "        에폭 종료 시점(on_validation_epoch_end)에서 사용.\n",
    "        \"\"\"\n",
    "        self._val_outputs.append(outputs)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        에폭 마지막에 축적한 self._val_outputs를 사용해\n",
    "        (1) val_loss 평균\n",
    "        (2) Recall@K\n",
    "        를 계산\n",
    "        \"\"\"\n",
    "        val_losses = torch.stack([o[\"val_loss\"] for o in self._val_outputs])\n",
    "        avg_val_loss = val_losses.mean()\n",
    "        self.log(\"val_loss\", avg_val_loss, prog_bar=True)\n",
    "\n",
    "        # Recall@K 계산\n",
    "        all_image_embeds = torch.cat([o[\"image_embeds\"] for o in self._val_outputs], dim=0)\n",
    "        all_text_embeds  = torch.cat([o[\"text_embeds\"] for o in self._val_outputs], dim=0)\n",
    "\n",
    "        similarity_matrix = all_text_embeds @ all_image_embeds.t()\n",
    "        recall_at_k = self.compute_recall(similarity_matrix, ks=[1,5,10])\n",
    "        for k, v in recall_at_k.items():\n",
    "            self.log(f\"val_recall@{k}\", v, prog_bar=True)\n",
    "        self.log(\"val_recall@5\", recall_at_k[5], prog_bar=True)  # 체크포인트 모니터\n",
    "\n",
    "        # 버퍼 비우기\n",
    "        self._val_outputs.clear()\n",
    "\n",
    "\n",
    "    # -----------------------\n",
    "    # Test\n",
    "    # -----------------------\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images = batch[\"image\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        image_embeds, text_embeds = self(images, input_ids, attention_mask)\n",
    "        return {\"image_embeds\": image_embeds, \"text_embeds\": text_embeds}\n",
    "\n",
    "    def on_test_batch_end(self, outputs, batch, batch_idx, dataloader_idx=0):\n",
    "        self.test_image_embeds.append(outputs[\"image_embeds\"])\n",
    "        self.test_text_embeds.append(outputs[\"text_embeds\"])\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        all_image_embeds = torch.cat(self.test_image_embeds, dim=0)\n",
    "        all_text_embeds  = torch.cat(self.test_text_embeds, dim=0)\n",
    "        similarity_matrix = all_text_embeds @ all_image_embeds.t()\n",
    "        recall_at_k = self.compute_recall(similarity_matrix, ks=[1,5,10])\n",
    "        for k, v in recall_at_k.items():\n",
    "            self.log(f\"test_recall@{k}\", v, prog_bar=True)\n",
    "        print(f\"[on_test_epoch_end] Test Recall: {recall_at_k}\")\n",
    "\n",
    "        self.test_image_embeds.clear()\n",
    "        self.test_text_embeds.clear()\n",
    "\n",
    "    # -----------------------\n",
    "    # 공통 함수\n",
    "    # -----------------------\n",
    "    def compute_recall(self, similarity_matrix, ks=[1,5,10]):\n",
    "        \"\"\"\n",
    "        similarity_matrix: (N, N) => row i: text i, col j: image j\n",
    "        대각선이 정답\n",
    "        \"\"\"\n",
    "        device = similarity_matrix.device\n",
    "        n = similarity_matrix.size(0)\n",
    "        ground_truth = torch.arange(n, device=device)\n",
    "\n",
    "        sorted_indices = similarity_matrix.argsort(dim=1, descending=True)\n",
    "        recall_scores = {}\n",
    "        for k in ks:\n",
    "            top_k = sorted_indices[:, :k]\n",
    "            match = (top_k == ground_truth.unsqueeze(1)).any(dim=1)\n",
    "            recall_scores[k] = match.float().mean().item()\n",
    "        return recall_scores\n",
    "\n",
    "    # -----------------------\n",
    "    # Optimizer\n",
    "    # -----------------------\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.trainer.max_epochs\n",
    "        )\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# DataModule 생성\n",
    "data_module = Flickr30KDataModule(\n",
    "    train_dataset_hf=train_dataset,\n",
    "    valid_dataset_hf=valid_dataset,\n",
    "    test_dataset_hf=test_dataset,\n",
    "    batch_size=128,  # contrastive learning 성능 위해 큰 배치 사용 가능\n",
    "    num_workers=4\n",
    ")\n",
    "data_module.setup(\"fit\")\n",
    "\n",
    "# 모델 초기화\n",
    "model = ImageTextLightningModel(\n",
    "    image_encoder_name=\"google/vit-base-patch16-224\",\n",
    "    text_encoder_name=\"roberta-base\",\n",
    "    embed_dim=256,\n",
    "    temperature=0.07,\n",
    "    learning_rate=5e-5\n",
    ")\n",
    "\n",
    "# 로거와 콜백 설정\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=\"ImageRetrieveLogs\",\n",
    "    name=\"ImageRetrieve_ValLoss_Recall\"\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_recall@5\",  # monitor: Recall@5\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=3,\n",
    "    mode=\"max\",        # recall은 클수록 좋다\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_recall@5\",\n",
    "    patience=5,\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=150,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\",\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [4]\n",
      "\n",
      "  | Name          | Type         | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | image_encoder | ViTModel     | 86.4 M | eval \n",
      "1 | text_encoder  | RobertaModel | 124 M  | eval \n",
      "2 | image_proj    | Linear       | 196 K  | train\n",
      "3 | text_proj     | Linear       | 196 K  | train\n",
      "-------------------------------------------------------\n",
      "211 M     Trainable params\n",
      "0         Non-trainable params\n",
      "211 M     Total params\n",
      "845.714   Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "455       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  26%|▎| 291/1132 [02:02<05:55,  2.37it/s, v_num=3, train_loss_step=0.194, val_loss=2.090, val_recall@1=0.115, val_recall@5=0.508, val_recall@10=0.654, train_loss_epo"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 및 체크포인트 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# 저장된 체크포인트 파일 경로\n",
    "checkpoint_path = \"/home/gpu_04/jw2020/ImageRetrieving/checkpoints/last.ckpt\"\n",
    "\n",
    "# 모델 로드\n",
    "model = ImageTextLightningModel.load_from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:05<00:00,  7.14it/s][on_test_epoch_end] Test Recall: {1: 0.11659999936819077, 5: 0.5389999747276306, 10: 0.6805999875068665}\n",
      "Testing DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:05<00:00,  7.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_recall@1       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.11659999936819077    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_recall@10       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6805999875068665     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_recall@5       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5389999747276306     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_recall@1      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.11659999936819077   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_recall@10      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6805999875068665    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_recall@5      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5389999747276306    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_recall@1': 0.11659999936819077,\n",
       "  'test_recall@5': 0.5389999747276306,\n",
       "  'test_recall@10': 0.6805999875068665}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "trainer.test(model, data_module)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
