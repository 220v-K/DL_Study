{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings & Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_SlTxSx8D5t2",
    "outputId": "002a9683-2e8e-464c-f2d1-e6c57bc07d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/bin/python\n",
      "Name: torch\n",
      "Version: 2.0.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /opt/anaconda3/lib/python3.11/site-packages\n",
      "Requires: filelock, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: \n",
      "cuda 버전: 12.1\n",
      "/usr/local/cuda-12.2/bin:/usr/local/cuda-12.2/bin:/usr/local/cuda-12.2/bin:/usr/local/cuda-12.2/bin:/home/gpu_04/.vscode-server/cli/servers/Stable-fabdb6a30b49f79a7aba0f2ad9df9b399473380f/server/bin/remote-cli:/opt/anaconda3/bin:/opt/anaconda3/bin:/usr/local/cuda-12.2/bin:/opt/anaconda3/condabin:/usr/local/cuda-12.2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/gpu_04/.vscode-server/data/User/globalStorage/github.copilot-chat/debugCommand\n",
      "/usr/local/cuda-12.2/lib64\n"
     ]
    }
   ],
   "source": [
    "# 2. 필요한 라이브러리 설치\n",
    "# %pip install -q kaggle\n",
    "\n",
    "# 3. Kaggle API 설정\n",
    "# import os\n",
    "\n",
    "!which python\n",
    "!pip show torch\n",
    "import torch\n",
    "print(\"cuda 버전:\", torch.version.cuda)\n",
    "!echo $PATH\n",
    "!echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 버전: 2.5.1+cu121\n",
      "CUDA 사용 가능 여부: True\n",
      "현재 디바이스: NVIDIA RTX A6000\n",
      "CUDA 버전: 12.1\n",
      "Epoch 6: 100%|█| 227/227 [1:18:46<00:00,  0.05it/s, v_num=0, train_loss_step=0.524, val_loss=1.030, val_recall@1=0.414, val_recall@5=0.745, val_recall@10=0.849, lr_layer_0=9.98e-6, lr_laye\n",
      "Allocated memory: 0.00 MB\n",
      "Reserved memory: 6.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch 버전:\", torch.__version__)\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "print(\"현재 디바이스:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"CUDA 버전:\", torch.version.cuda if torch.cuda.is_available() else \"None\")\n",
    "\n",
    "# GPU 메모리 단편화 문제 완화\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# GPU 캐시 비우기\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# 1. 불필요한 변수 삭제\n",
    "# del variable\n",
    "\n",
    "# 2. 가비지 컬렉터 실행\n",
    "gc.collect()\n",
    "\n",
    "# 3. PyTorch 캐시 메모리 해제\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 4. 메모리 사용 상태 출력\n",
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Reserved memory: {torch.cuda.memory_reserved() / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Flickr30k 데이터셋 다운로드\n",
    "dataset = load_dataset(\"nlphuji/flickr30k\")\n",
    "train_dataset = dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "valid_dataset = dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "test_dataset = dataset.filter(lambda x: x[\"split\"] == \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
      "        num_rows: 29000\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
      "        num_rows: 1014\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(valid_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel, RobertaModel, RobertaTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 시드 고정\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet Structure (Lightning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모든 데이터셋 쌍 (이미지-캡션 5개) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr30KSingleCaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    한 이미지당 캡션 1개만(무작위) 사용\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_dataset, tokenizer, image_transform, max_length=64):\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_transform = image_transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        pil_image = data[\"image\"]\n",
    "        captions = data[\"caption\"]\n",
    "        caption = random.choice(captions)  # 5개 중 1개 무작위 선택\n",
    "\n",
    "        # 이미지 전처리\n",
    "        image = self.image_transform(pil_image)\n",
    "\n",
    "        # 텍스트 토큰화\n",
    "        tokenized = self.tokenizer(\n",
    "            caption,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel, RobertaModel, RobertaTokenizer\n",
    "\n",
    "class Flickr30KDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 train_dataset_hf,\n",
    "                 valid_dataset_hf,\n",
    "                 test_dataset_hf,\n",
    "                 batch_size=32,\n",
    "                 num_workers=4):\n",
    "        super().__init__()\n",
    "        self.train_dataset_hf = train_dataset_hf\n",
    "        self.valid_dataset_hf = valid_dataset_hf\n",
    "        self.test_dataset_hf = test_dataset_hf\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5),\n",
    "                                 std=(0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = Flickr30KSingleCaptionDataset(\n",
    "                self.train_dataset_hf[\"test\"],\n",
    "                tokenizer=self.tokenizer,\n",
    "                image_transform=self.image_transform\n",
    "            )\n",
    "            self.valid_dataset = Flickr30KSingleCaptionDataset(\n",
    "                self.valid_dataset_hf[\"test\"],\n",
    "                tokenizer=self.tokenizer,\n",
    "                image_transform=self.image_transform\n",
    "            )\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = Flickr30KSingleCaptionDataset(\n",
    "                self.test_dataset_hf[\"test\"],\n",
    "                tokenizer=self.tokenizer,\n",
    "                image_transform=self.image_transform\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valid_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure (Lightning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextLightningModel(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 image_encoder_name=\"google/vit-base-patch16-224\",\n",
    "                 text_encoder_name=\"roberta-base\",\n",
    "                 embed_dim=256,\n",
    "                 temperature=0.07,\n",
    "                 learning_rate=5e-5,\n",
    "                 vit_train_layers=2,        # 학습할 비전 트랜스포머 레이어 수\n",
    "                 roberta_train_layers=2):   # 학습할 로버타 레이어 수\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # 1) Image Encoder (ViT)\n",
    "        self.image_encoder = ViTModel.from_pretrained(image_encoder_name)\n",
    "        # 2) Text Encoder (RoBERTa)\n",
    "        self.text_encoder = RobertaModel.from_pretrained(text_encoder_name)\n",
    "\n",
    "        # Projection layers\n",
    "        self.image_proj = nn.Linear(768, embed_dim)\n",
    "        self.text_proj = nn.Linear(768, embed_dim)\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Validation 시 batch별 결과 임시 저장\n",
    "        self._val_outputs = []\n",
    "        # Test 시 batch별 결과 임시 저장\n",
    "        self.test_image_embeds = []\n",
    "        self.test_text_embeds = []\n",
    "\n",
    "        # ------------------\n",
    "        # Freeze + Unfreeze\n",
    "        # ------------------\n",
    "        self.freeze_vit_layers(train_layers=vit_train_layers)\n",
    "        self.freeze_roberta_layers(train_layers=roberta_train_layers)\n",
    "\n",
    "    def freeze_vit_layers(self, train_layers=12):\n",
    "        \"\"\"\n",
    "        ViT의 마지막 train_layers개 레이어만 Fine-tuning하고, 나머지는 동결\n",
    "        \"\"\"\n",
    "        # (1) 전체 파라미터 동결\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # (2) total 레이어 수 확인 (예: vit-base-patch16-224는 12층)\n",
    "        total_layers = len(self.image_encoder.encoder.layer)\n",
    "        # (3) 마지막 N개의 레이어만 unfreeze\n",
    "        for layer_idx in range(total_layers - train_layers, total_layers):\n",
    "            for param in self.image_encoder.encoder.layer[layer_idx].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # Pooler나 LayerNorm 등 추가로 학습해야 할 부분 있으면 여기서 풀어줄 수 있음\n",
    "        if hasattr(self.image_encoder, \"layernorm\"):\n",
    "            for param in self.image_encoder.layernorm.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def freeze_roberta_layers(self, train_layers=12):\n",
    "        \"\"\"\n",
    "        RoBERTa의 마지막 train_layers개 레이어만 Fine-tuning\n",
    "        \"\"\"\n",
    "        # (1) 전체 파라미터 동결\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # (2) roberta encoder.layer 총 12층\n",
    "        total_layers = len(self.text_encoder.encoder.layer)\n",
    "        # (3) 마지막 N개 레이어 unfreeze\n",
    "        for layer_idx in range(total_layers - train_layers, total_layers):\n",
    "            for param in self.text_encoder.encoder.layer[layer_idx].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # pooler or LMHead 등 추가로 학습하려면 풀어줄 수 있음\n",
    "        if hasattr(self.text_encoder, \"pooler\"):\n",
    "            for param in self.text_encoder.pooler.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # ------------------\n",
    "    # forward\n",
    "    # ------------------\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # --- ViT ---\n",
    "        image_outputs = self.image_encoder(pixel_values=images)\n",
    "        # last_hidden_state[:, 0, :] → [CLS]\n",
    "        image_cls = image_outputs.last_hidden_state[:, 0, :]\n",
    "        image_embeds = self.image_proj(image_cls)\n",
    "        image_embeds = F.normalize(image_embeds, p=2, dim=-1)\n",
    "\n",
    "        # --- RoBERTa ---\n",
    "        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_cls = text_outputs.last_hidden_state[:, 0, :]\n",
    "        text_embeds = self.text_proj(text_cls)\n",
    "        text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n",
    "\n",
    "        return image_embeds, text_embeds\n",
    "\n",
    "    # ------------------\n",
    "    # compute_contrastive_loss\n",
    "    # ------------------\n",
    "    def compute_contrastive_loss(self, image_embeds, text_embeds):\n",
    "        logits_per_image = image_embeds @ text_embeds.t() / self.temperature\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        batch_size = image_embeds.size(0)\n",
    "        labels = torch.arange(batch_size, device=self.device)\n",
    "\n",
    "        loss_i = F.cross_entropy(logits_per_image, labels)\n",
    "        loss_t = F.cross_entropy(logits_per_text, labels)\n",
    "        loss = (loss_i + loss_t) / 2.0\n",
    "        return loss\n",
    "\n",
    "    # ------------------\n",
    "    # train step\n",
    "    # ------------------\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images = batch[\"image\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "        image_embeds, text_embeds = self(images, input_ids, attention_mask)\n",
    "        loss = self.compute_contrastive_loss(image_embeds, text_embeds)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_start(self):\n",
    "        \"\"\"Epoch 시작 시 현재 Learning Rate 확인\"\"\"\n",
    "        optimizer = self.optimizers()\n",
    "        for i, param_group in enumerate(optimizer.param_groups):\n",
    "            lr = param_group[\"lr\"]\n",
    "            self.log(f\"lr_layer_{i}\", lr, prog_bar=True)\n",
    "\n",
    "    # ------------------\n",
    "    # validation step\n",
    "    # ------------------\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images = batch[\"image\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        image_embeds, text_embeds = self(images, input_ids, attention_mask)\n",
    "\n",
    "        val_loss = self.compute_contrastive_loss(image_embeds, text_embeds)\n",
    "        self.log(\"val_loss_step\", val_loss, prog_bar=False)\n",
    "\n",
    "        return {\n",
    "            \"val_loss\": val_loss,\n",
    "            \"image_embeds\": image_embeds,\n",
    "            \"text_embeds\": text_embeds\n",
    "        }\n",
    "\n",
    "    def on_validation_batch_end(self, outputs, batch, batch_idx, dataloader_idx=0):\n",
    "        self._val_outputs.append(outputs)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # 전체 val batch 결과 종합\n",
    "        val_losses = torch.stack([o[\"val_loss\"] for o in self._val_outputs])\n",
    "        avg_val_loss = val_losses.mean()\n",
    "        self.log(\"val_loss\", avg_val_loss, prog_bar=True)\n",
    "\n",
    "        # Recall@K 계산\n",
    "        all_image_embeds = torch.cat([o[\"image_embeds\"] for o in self._val_outputs], dim=0)\n",
    "        all_text_embeds  = torch.cat([o[\"text_embeds\"]  for o in self._val_outputs], dim=0)\n",
    "        similarity_matrix = all_text_embeds @ all_image_embeds.t()\n",
    "        recall_at_k = self.compute_recall(similarity_matrix, ks=[1,5,10])\n",
    "\n",
    "        for k, v in recall_at_k.items():\n",
    "            self.log(f\"val_recall@{k}\", v, prog_bar=True)\n",
    "        self.log(\"val_recall@5\", recall_at_k[5], prog_bar=True)\n",
    "\n",
    "        self._val_outputs.clear()\n",
    "\n",
    "    # ------------------\n",
    "    # test step\n",
    "    # ------------------\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images = batch[\"image\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        image_embeds, text_embeds = self(images, input_ids, attention_mask)\n",
    "        return {\"image_embeds\": image_embeds, \"text_embeds\": text_embeds}\n",
    "\n",
    "    def on_test_batch_end(self, outputs, batch, batch_idx, dataloader_idx=0):\n",
    "        self.test_image_embeds.append(outputs[\"image_embeds\"])\n",
    "        self.test_text_embeds.append(outputs[\"text_embeds\"])\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        all_image_embeds = torch.cat(self.test_image_embeds, dim=0)\n",
    "        all_text_embeds  = torch.cat(self.test_text_embeds, dim=0)\n",
    "        similarity_matrix = all_text_embeds @ all_image_embeds.t()\n",
    "        recall_at_k = self.compute_recall(similarity_matrix, ks=[1,5,10])\n",
    "        for k, v in recall_at_k.items():\n",
    "            self.log(f\"test_recall@{k}\", v, prog_bar=True)\n",
    "        print(f\"[on_test_epoch_end] Test Recall: {recall_at_k}\")\n",
    "\n",
    "        self.test_image_embeds.clear()\n",
    "        self.test_text_embeds.clear()\n",
    "\n",
    "    # ------------------\n",
    "    # recall\n",
    "    # ------------------\n",
    "    def compute_recall(self, similarity_matrix, ks=[1,5,10]):\n",
    "        device = similarity_matrix.device\n",
    "        n = similarity_matrix.size(0)\n",
    "        ground_truth = torch.arange(n, device=device)\n",
    "        sorted_indices = similarity_matrix.argsort(dim=1, descending=True)\n",
    "\n",
    "        recall_scores = {}\n",
    "        for k in ks:\n",
    "            top_k = sorted_indices[:, :k]\n",
    "            match = (top_k == ground_truth.unsqueeze(1)).any(dim=1)\n",
    "            recall_scores[k] = match.float().mean().item()\n",
    "        return recall_scores\n",
    "\n",
    "    # ------------------\n",
    "    # optimizer\n",
    "    # ------------------\n",
    "    # def configure_optimizers(self):\n",
    "    #     # optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "    #     ## Layer별 다른 LR 설정 (projection layer에 더 큰 lr)\n",
    "    #     optimizer = torch.optim.AdamW([\n",
    "    #         {\"params\": model.image_encoder.encoder.layer[-2:].parameters(), \"lr\": 1e-5},\n",
    "    #         {\"params\": model.text_encoder.encoder.layer[-2:].parameters(), \"lr\": 1e-5},\n",
    "    #         {\"params\": model.image_proj.parameters(), \"lr\": 5e-5},\n",
    "    #         {\"params\": model.text_proj.parameters(), \"lr\": 5e-5}\n",
    "    #     ])\n",
    "    #     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    #         optimizer, T_max=self.trainer.max_epochs\n",
    "    #     )\n",
    "    #     return [optimizer], [scheduler]\n",
    "    def configure_optimizers(self):\n",
    "        base_lr = 1e-5  # 기본 Learning Rate\n",
    "        layerwise_decay = 0.9  # Layer-wise decay 비율\n",
    "\n",
    "        optimizer_params = []\n",
    "\n",
    "        # ViT 마지막 6개 레이어에 서로 다른 LR 적용\n",
    "        total_vit_layers = len(self.image_encoder.encoder.layer)\n",
    "        for i, layer_idx in enumerate(range(total_vit_layers - 12, total_vit_layers)):\n",
    "            lr = base_lr * (layerwise_decay ** i)  # layer-wise decay 적용\n",
    "            optimizer_params.append({\"params\": self.image_encoder.encoder.layer[layer_idx].parameters(), \"lr\": lr})\n",
    "\n",
    "        # RoBERTa 마지막 6개 레이어에 서로 다른 LR 적용\n",
    "        total_roberta_layers = len(self.text_encoder.encoder.layer)\n",
    "        for i, layer_idx in enumerate(range(total_roberta_layers - 12, total_roberta_layers)):\n",
    "            lr = base_lr * (layerwise_decay ** i)\n",
    "            optimizer_params.append({\"params\": self.text_encoder.encoder.layer[layer_idx].parameters(), \"lr\": lr})\n",
    "\n",
    "        # Projection Layer는 별도로 더 큰 학습률 적용 (기본적으로 새로 학습되는 부분)\n",
    "        optimizer_params.append({\"params\": self.image_proj.parameters(), \"lr\": 5e-5})\n",
    "        optimizer_params.append({\"params\": self.text_proj.parameters(), \"lr\": 5e-5})\n",
    "\n",
    "        optimizer = torch.optim.AdamW(optimizer_params, lr=base_lr, weight_decay=1e-4)\n",
    "\n",
    "        # Learning Rate Scheduler (Cosine Decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.trainer.max_epochs, eta_min=1e-7)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# DataModule 생성\n",
    "data_module = Flickr30KDataModule(\n",
    "    train_dataset_hf=train_dataset,\n",
    "    valid_dataset_hf=valid_dataset,\n",
    "    test_dataset_hf=test_dataset,\n",
    "    batch_size=128,  # contrastive learning -> batch size\n",
    "    num_workers=4\n",
    ")\n",
    "data_module.setup(\"fit\")\n",
    "\n",
    "# 모델 초기화\n",
    "model = ImageTextLightningModel(\n",
    "    image_encoder_name=\"google/vit-base-patch16-224\",\n",
    "    text_encoder_name=\"roberta-base\",\n",
    "    embed_dim=256,\n",
    "    temperature=0.07,\n",
    "    learning_rate=1e-5,\n",
    "    vit_train_layers=12,        # ViT 마지막 2개 레이어만 학습\n",
    "    roberta_train_layers=12     # RoBERTa 마지막 2개 레이어만 학습\n",
    ")\n",
    "\n",
    "# 로거와 콜백 설정\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=\"ImageRetrieveLogs\",\n",
    "    name=\"ImageRetrieve_6th\"\n",
    ")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_recall@5\",\n",
    "    mode=\"max\",\n",
    "    dirpath=\"checkpoints_7th\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=3,\n",
    "    save_last=True\n",
    ")\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_recall@5\",\n",
    "    patience=20,\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,   # 에폭 수 (하드웨어 성능 따라 조정)\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\",\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [4]\n",
      "\n",
      "  | Name          | Type         | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | image_encoder | ViTModel     | 86.4 M | eval \n",
      "1 | text_encoder  | RobertaModel | 124 M  | eval \n",
      "2 | image_proj    | Linear       | 196 K  | train\n",
      "3 | text_proj     | Linear       | 196 K  | train\n",
      "-------------------------------------------------------\n",
      "86.0 M    Trainable params\n",
      "125 M     Non-trainable params\n",
      "211 M     Total params\n",
      "845.714   Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "455       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  21%|██████████████████████▌                                                                                      | 47/227 [00:13<00:51,  3.49it/s, v_num=1, train_loss_step=3.430]"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 및 체크포인트 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# 저장된 체크포인트 파일 경로\n",
    "checkpoint_path = \"/home/gpu_04/jw2020/ImageRetrieving/checkpoints/best-checkpoint-v2.ckpt\"\n",
    "\n",
    "# 모델 로드\n",
    "model = ImageTextLightningModel.load_from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:01<00:00,  5.64it/s][on_test_epoch_end] Test Recall: {1: 0.49500003457069397, 5: 0.8080000281333923, 10: 0.8950000405311584}\n",
      "Testing DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:01<00:00,  5.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_recall@1       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.49500003457069397    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_recall@10       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8950000405311584     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_recall@5       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8080000281333923     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_recall@1      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.49500003457069397   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_recall@10      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8950000405311584    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_recall@5      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8080000281333923    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_recall@1': 0.49500003457069397,\n",
       "  'test_recall@5': 0.8080000281333923,\n",
       "  'test_recall@10': 0.8950000405311584}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "trainer.test(model, data_module)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
